{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generación de Embeddings**\n",
    "\n",
    "Este notebook se encargará de generar embeddings para cada producto utilizando sentence-transformers y almacenarlos en Elasticsearch. Además, se genera un respaldo del DataFrame con embeddings  para futuras consultas o análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conexion y carga**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Conectar a Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"amazon_products\"\n",
    "\n",
    "#  Cargar el modelo preentrenado\n",
    "# Se deja de usar por el alto tiempo de procesamiento, se puede usar esta opcion con recursos elevados de procesamiento\n",
    "#model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "#  Cargar el modelo optimizado\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "#  Cargar de df procesado\n",
    "df_path = \"../data/processed/df_pro.csv\" \n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación generaremos los embeddings usando el modelo, para asegurarse de no tener problemas de recursos se usará batch_size. En las ejecuciones de prueba el modelo optimizado tomó 7 min en ejecutarse.\n",
    "\n",
    "**Si se descarga el archivo parquet con los embeddings ya generados y se los coloca en la carpeta correspondiente, se puede saltar hasta la celda 6 de este nb.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bfe20a0aca4820b96aab6151c1687b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'name' a lista\n",
    "product_names = df['name'].tolist()\n",
    "# Generar embeddings en batches para optimizar\n",
    "batch_size = 128  \n",
    "embeddings = model.encode(product_names, batch_size=batch_size, show_progress_bar=True)\n",
    "df['embedding'] = [emb.tolist() for emb in embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comprobando embeddings generados antes de indexarlos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de embedding:\n",
      "[-0.1474011242389679, -0.0034425125923007727, -0.0014247492654249072, 0.02615339495241642, 0.028749532997608185, -0.062161415815353394, -0.02965972200036049, 0.025048580020666122, -0.07818062603473663, -0.05018840357661247]...\n",
      "Tamaño del embedding: 384\n"
     ]
    }
   ],
   "source": [
    "# Comprobación de generacion de embedding\n",
    "ejemplo_embedding = df['embedding'].iloc[10]\n",
    "print(f\"Ejemplo de embedding:\\n{ejemplo_embedding[:10]}...\")  # Mostrar los primeros 10 números\n",
    "print(f\"Tamaño del embedding: {len(ejemplo_embedding)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings guardados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Guardar el DataFrame con embeddings en un archivo parquet\n",
    "df.to_parquet(\"../data/processed/amazon_products_with_embeddings.parquet\")\n",
    "print(\"Embeddings guardados exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Codigo para cargar embeddings y no generarlos\n",
    "df= pd.read_parquet(\"../data/processed/amazon_products_with_embeddings.parquet\")\n",
    "# Convertir cada embedding a lista\n",
    "df['embedding'] = df['embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduccion de dimensiones(opcional)(no se usó en app.py)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desarrolló una función para reducir la dimensionalidad del primer modelo que se probó (all-mpnet-base-v2), pero se decidió que era una mejor estrategia cambiar de modelo a uno más ligero para reducir tiempos de generación e indexación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "def aplicar_umap(df, n_dim=200):\n",
    "    \"\"\"\n",
    "    Reduce la dimensionalidad de los embeddings usando UMAP.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con la columna 'embedding' que contiene los embeddings originales.\n",
    "        n_dim (int): Número de dimensiones deseadas tras la reducción (por defecto 200).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con la columna 'embedding' actualizada con los embeddings reducidos.\n",
    "    \"\"\"\n",
    "    # Convertir los embeddings a un array NumPy\n",
    "    embeddings = np.array(df['embedding'].tolist())\n",
    "    \n",
    "    ##embeddings=df['embedding']\n",
    "    # Aplicar UMAP\n",
    "    umap_model = umap.UMAP(n_components=n_dim, random_state=42)\n",
    "    reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "    \n",
    "    # Reemplazar la columna de embeddings\n",
    "    df['embedding'] = reduced_embeddings.tolist()\n",
    "    \n",
    "    print(f\"UMAP aplicado exitosamente. Dimensiones reducidas a {n_dim}.\")\n",
    "    return df\n",
    "df_red = aplicar_umap(df, n_dim=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verificación de formatos y estructura previo a la indexación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_type\n",
      "list    490965\n",
      "Name: count, dtype: int64\n",
      "embedding_length\n",
      "384    490965\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificar tipo de dato y longitud de cada embedding\n",
    "df['embedding_type'] = df['embedding'].apply(lambda x: type(x).__name__)\n",
    "df['embedding_length'] = df['embedding'].apply(lambda x: len(x) if isinstance(x, list) else None)\n",
    "\n",
    "# Mostrar un resumen de los tipos de datos encontrados\n",
    "print(df['embedding_type'].value_counts())\n",
    "\n",
    "#  Mostrar un resumen de las longitudes de embeddings encontradas\n",
    "print(df['embedding_length'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Todos los registros tienen embeddings válidos.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si todos los embeddings son listas de floats\n",
    "error_count = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    embedding = row['embedding']\n",
    "    if not isinstance(embedding, list) or not all(isinstance(x, float) for x in embedding):\n",
    "        print(f\" Registro inválido en el índice {index}. Tipo de embedding: {type(embedding)}\")\n",
    "        error_count += 1\n",
    "\n",
    "if error_count == 0:\n",
    "    print(\" Todos los registros tienen embeddings válidos.\")\n",
    "else:\n",
    "    print(f\" Se encontraron {error_count} registros inválidos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"amazon_products_embeddings\": {\n",
      "    \"mappings\": {\n",
      "      \"properties\": {\n",
      "        \"embedding\": {\n",
      "          \"type\": \"dense_vector\",\n",
      "          \"dims\": 384,\n",
      "          \"index\": true,\n",
      "          \"similarity\": \"cosine\",\n",
      "          \"index_options\": {\n",
      "            \"type\": \"int8_hnsw\",\n",
      "            \"m\": 16,\n",
      "            \"ef_construction\": 100\n",
      "          }\n",
      "        },\n",
      "        \"link\": {\n",
      "          \"type\": \"keyword\"\n",
      "        },\n",
      "        \"main_category\": {\n",
      "          \"type\": \"keyword\"\n",
      "        },\n",
      "        \"name\": {\n",
      "          \"type\": \"text\"\n",
      "        },\n",
      "        \"ratings\": {\n",
      "          \"type\": \"float\"\n",
      "        },\n",
      "        \"sub_category\": {\n",
      "          \"type\": \"keyword\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Comprobar configuración del índice\n",
    "index_info = es.indices.get_mapping(index=index_name)\n",
    "index_info_dict = index_info.body if hasattr(index_info, 'body') else index_info  # Convertir a diccionario si es necesario\n",
    "print(json.dumps(index_info_dict, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creación del indice con Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice 'amazon_products_embeddings' eliminado exitosamente.\n",
      " Índice 'amazon_products_embeddings' creado exitosamente con dimensión 384.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Establecer la conexión con Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "embedding_dim = len(df['embedding'].iloc[0])\n",
    "\n",
    "# Nombre del índice\n",
    "index_name = \"amazon_products_embeddings\"\n",
    "\n",
    "# Configuración del índice\n",
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"text\"},\n",
    "            \"main_category\": {\"type\": \"keyword\"},\n",
    "            \"sub_category\": {\"type\": \"keyword\"},\n",
    "            \"link\": {\"type\": \"keyword\"},\n",
    "            \"ratings\": {\"type\": \"float\"},\n",
    "            \"embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": embedding_dim   # se asigna dinámicamente\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "#  Eliminar el índice si ya existe\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Índice '{index_name}' eliminado exitosamente.\")\n",
    "\n",
    "# Crear el índice nuevamente con la nueva configuración\n",
    "es.indices.create(index=index_name, body=index_config)\n",
    "print(f\" Índice '{index_name}' creado exitosamente con dimensión {embedding_dim}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "import json\n",
    "\n",
    "def indexar_datos_embeddings_v2(es, df, index_name, chunk_size=2500, embedding_dim=384):\n",
    "    # Mostrar cantidad de filas antes de la limpieza\n",
    "    print(f\" Total de filas antes de limpiar: {len(df)}\")\n",
    "    \n",
    "    # Limpiar el DataFrame y reemplazar NaNs por None\n",
    "    df_cleaned = df[['name', 'main_category', 'sub_category', 'ratings', 'no_of_ratings', \n",
    "                     'discount_price_dolares', 'actual_price_dolares', 'embedding']].copy()\n",
    "    \n",
    "    # Filtrar filas con embeddings no válidos o incompletos\n",
    "    df_cleaned = df_cleaned[df_cleaned['embedding'].apply(lambda x: isinstance(x, list) and len(x) == embedding_dim)]\n",
    "    df_cleaned = df_cleaned.where(pd.notna(df_cleaned), None)\n",
    "    \n",
    "    # Mostrar cantidad de filas después de la limpieza\n",
    "    print(f\" Total de filas después de limpiar: {len(df_cleaned)}\")\n",
    "    \n",
    "    # Conversión explícita de columnas\n",
    "    df_cleaned['ratings'] = pd.to_numeric(df_cleaned['ratings'], errors='coerce').fillna(0)\n",
    "    df_cleaned['no_of_ratings'] = pd.to_numeric(df_cleaned['no_of_ratings'], errors='coerce').fillna(0)\n",
    "    df_cleaned['discount_price_dolares'] = pd.to_numeric(df_cleaned['discount_price_dolares'], errors='coerce').fillna(0)\n",
    "    df_cleaned['actual_price_dolares'] = pd.to_numeric(df_cleaned['actual_price_dolares'], errors='coerce').fillna(0)\n",
    "\n",
    "    # Índices de documentos fallidos\n",
    "    failed_documents = []\n",
    "\n",
    "    # Indexar en lotes\n",
    "    total_indexed = 0\n",
    "    total_failed = 0\n",
    "\n",
    "    for start_idx in range(0, len(df_cleaned), chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, len(df_cleaned))\n",
    "        batch = df_cleaned.iloc[start_idx:end_idx].to_dict(orient=\"records\")\n",
    "        \n",
    "        # Crear el formato adecuado para la indexación\n",
    "        bulk_data = [\n",
    "            {\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": {\n",
    "                    \"name\": record[\"name\"],\n",
    "                    \"main_category\": record[\"main_category\"],\n",
    "                    \"sub_category\": record[\"sub_category\"],\n",
    "                    \"ratings\": record[\"ratings\"],\n",
    "                    \"no_of_ratings\": record[\"no_of_ratings\"],\n",
    "                    \"discount_price_dolares\": record[\"discount_price_dolares\"],\n",
    "                    \"actual_price_dolares\": record[\"actual_price_dolares\"],\n",
    "                    \"embedding\": record[\"embedding\"]\n",
    "                }\n",
    "            }\n",
    "            for record in batch\n",
    "        ]\n",
    "        \n",
    "        # Intentar indexar y capturar errores\n",
    "        try:\n",
    "            success, failed = helpers.bulk(es, bulk_data, stats_only=True)\n",
    "            total_indexed += success\n",
    "            total_failed += failed\n",
    "            #print(f\" Lote {start_idx // chunk_size + 1} indexado correctamente: {success} documentos.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            #print(f\" Error al indexar el lote {start_idx // chunk_size + 1}: {e}\")\n",
    "            failed_documents.append({\n",
    "                \"lote\": start_idx // chunk_size + 1,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Mostrar resultados finales\n",
    "    print(f\"\\n Indexación completada. Total indexados: {total_indexed}. Total fallidos: {total_failed}.\")\n",
    "    \n",
    "    # Guardar errores en un archivo JSON si existen\n",
    "    if failed_documents:\n",
    "        with open('errores_indexacion.json', 'w') as f:\n",
    "            json.dump(failed_documents, f, indent=4)\n",
    "        print(\" Los errores se han guardado en 'errores_indexacion.json'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total de filas antes de limpiar: 490965\n",
      " Total de filas después de limpiar: 490965\n",
      "\n",
      " Indexación completada. Total indexados: 190000. Total fallidos: 0.\n",
      " Los errores se han guardado en 'errores_indexacion.json'.\n"
     ]
    }
   ],
   "source": [
    "indexar_datos_embeddings_v2(es, df, index_name, chunk_size=2500, embedding_dim=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función para realizar consultas usando metricas de similitud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buscar_productos_similares(es, index_name, product_name, top_k=5):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda en Elasticsearch usando embeddings para encontrar productos similares.\n",
    "    (Esta función utiliza cosineSimilarity de Elasticsearch para mejor rendimiento).\n",
    "    \n",
    "    Args:\n",
    "        es (Elasticsearch): Conexión a Elasticsearch.\n",
    "        index_name (str): Nombre del índice en Elasticsearch.\n",
    "        product_name (str): Nombre del producto a buscar.\n",
    "        top_k (int): Número de productos similares a devolver.\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de productos similares encontrados.\n",
    "    \"\"\"\n",
    "    # Buscar el producto original por nombre en Elasticsearch\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"name\": product_name\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=index_name, body=query)\n",
    "    \n",
    "    if not response[\"hits\"][\"hits\"]:\n",
    "        print(f\" Producto '{product_name}' no encontrado en el índice.\")\n",
    "        return []\n",
    "    \n",
    "    # Obtener el embedding del producto encontrado\n",
    "    producto = response[\"hits\"][\"hits\"][0][\"_source\"]\n",
    "    product_embedding = producto[\"embedding\"]\n",
    "    \n",
    "    # Construir la consulta de similitud con embeddings\n",
    "    search_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": product_embedding}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Realizar la búsqueda en Elasticsearch\n",
    "    similar_products = es.search(index=index_name, body=search_body)\n",
    "    \n",
    "    # Procesar resultados\n",
    "    results = [\n",
    "        {\n",
    "            \"name\": hit[\"_source\"][\"name\"],\n",
    "            \"main_category\": hit[\"_source\"][\"main_category\"],\n",
    "            \"sub_category\": hit[\"_source\"][\"sub_category\"],\n",
    "            \"ratings\": hit[\"_source\"][\"ratings\"],\n",
    "            \"no_of_ratings\": hit[\"_source\"][\"no_of_ratings\"],\n",
    "            \"discount_price_dolares\": hit[\"_source\"][\"discount_price_dolares\"],\n",
    "            \"actual_price_dolares\": hit[\"_source\"][\"actual_price_dolares\"],\n",
    "            \"score\": hit[\"_score\"]\n",
    "        }\n",
    "        for hit in similar_products[\"hits\"][\"hits\"]\n",
    "    ]\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba de funcionalidad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = buscar_productos_similares(es, index_name=\"amazon_products_embeddings\", product_name=\"Echo Dot\", top_k=5)\n",
    "for producto in resultados:\n",
    "    print(f\"Producto: {producto['name']}, Score: {producto['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = buscar_productos_similares(es, index_name=\"amazon_products_embeddings\", product_name=\"AirPods\", top_k=5)\n",
    "for producto in resultados:\n",
    "    print(f\"Producto: {producto['name']}, Score: {producto['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = buscar_productos_similares(es, index_name=\"amazon_products_embeddings\", product_name=\"Yoga Mat\", top_k=5)\n",
    "for producto in resultados:\n",
    "    print(f\"Producto: {producto['name']}, Score: {producto['score']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
