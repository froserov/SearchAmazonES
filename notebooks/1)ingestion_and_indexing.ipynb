{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importacion de dataframe, preprocesamiento y seteo de indice en E.S**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿De que trata el caso de estudio?**\n",
    "\n",
    "El caso presenta datasets obtenidos a traves de scraping del sitio web de Amazon. En la fuente constan 142 archivos correspondientes a compras de productos separadas por cada categoria, así como el dataset combinado de todas las categorias con el nombre Amazon-Producs.csv,que contiene la información de ventas de 551 mil productos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/Amazon-Products.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 551585 entries, 0 to 551584\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   Unnamed: 0      551585 non-null  int64 \n",
      " 1   name            551585 non-null  object\n",
      " 2   main_category   551585 non-null  object\n",
      " 3   sub_category    551585 non-null  object\n",
      " 4   image           551585 non-null  object\n",
      " 5   link            551585 non-null  object\n",
      " 6   ratings         375791 non-null  object\n",
      " 7   no_of_ratings   375791 non-null  object\n",
      " 8   discount_price  490422 non-null  object\n",
      " 9   actual_price    533772 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 42.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primeras observaciones**\n",
    "\n",
    "- Los precios estan expresados en Rupias\n",
    "- Existen productos que no cuentan con información de ratings de compras\n",
    "- Cerca de 40 mil productos no cuentan con precios de descuento\n",
    "- La primera columna del ds funciona como un conteo de filas\n",
    "- Existen productos que podemos considerar como duplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transformación de precios completada exitosamente.\n"
     ]
    }
   ],
   "source": [
    "#  Transformacion de precios a dolares\n",
    "\n",
    "def convertir_a_numero(valor):\n",
    "    if isinstance(valor, str):  # Verificar si el valor es una cadena\n",
    "        valor = valor.replace('₹', '').replace(',', '').strip()  # Eliminar símbolos y comas\n",
    "        try:\n",
    "            return float(valor)\n",
    "        except ValueError:\n",
    "            return None  # Devolver None si la conversión falla\n",
    "    return None  # Devolver None si el valor no es una cadena\n",
    "\n",
    "#  Aplicar la función a las columnas correspondientes\n",
    "df['discount_price'] = df['discount_price'].apply(convertir_a_numero)\n",
    "df['actual_price'] = df['actual_price'].apply(convertir_a_numero)\n",
    "\n",
    "# Tasa de conversión de Rupias a Dólares (puedes actualizar esta tasa si lo deseas)\n",
    "tasa_conversion = 0.012  # Aproximadamente 1 INR = 0.012 USD\n",
    "\n",
    "#  Crear columnas con precios en dólares y redondear a 2 decimales\n",
    "df['discount_price_dolares'] = df['discount_price'].apply(lambda x: round(x * tasa_conversion, 2) if x is not None else None)\n",
    "df['actual_price_dolares'] = df['actual_price'].apply(lambda x: round(x * tasa_conversion, 2) if x is not None else None)\n",
    "\n",
    "print(\" Transformación de precios completada exitosamente.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminando columnas innecesarias y revisando valores perdidos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eliminando columna con conteo de filas\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporción de datos faltantes por columna:\n",
      "ratings                   31.87\n",
      "no_of_ratings             31.87\n",
      "discount_price            11.09\n",
      "actual_price               3.23\n",
      "discount_price_dolares    11.09\n",
      "actual_price_dolares       3.23\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#  Proporción de datos faltantes por columna (en %)\n",
    "missing_percentage = (df.isnull().mean() * 100).round(2)\n",
    "missing_percentage = missing_percentage[missing_percentage > 0]\n",
    "if not missing_percentage.empty:\n",
    "    print(\"Proporción de datos faltantes por columna:\")\n",
    "    print(missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por el momento no se modificarán los valores perdidos, ya que se observa que la mayor parte vienen de los ratings. Posteriormente se le datá tratamiento a estos casos al momento de indexarlos a E.S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Manejo de productos duplicados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si consideramos a todos los campos disponibles en la base se observa que **NO** existen duplicados, pero si por ejemplo, tomamos en cuenta la combinación de nombre, categoria, rating, precio de descuento y precio actual se puede observar que los duplicados son más de 60 mil. En este caso, se decide **ELIMINAR** estos registros, ya que solo estarian diferenciados por caracteristicas como su link o su imagen, y pueden estar causados por **duplicidad de ciertos productos en más de una categoria** \n",
    "\n",
    "La eliminación de este tipo de registros tambien nos ayudará a obtener mejores resultados en pasos posteriores como con el trabajo con embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Productos duplicados detectados: 60620\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por combinación única de columnas\n",
    "df= df.copy()\n",
    "duplicados = df.duplicated(subset=[\"name\", \"main_category\", \"sub_category\", \"ratings\",\"no_of_ratings\",\n",
    "                                           \"discount_price\",\"actual_price\"\n",
    "                                           ], keep=False)\n",
    "\n",
    "# Mostrar número de duplicados\n",
    "print(f\" Productos duplicados detectados: {duplicados.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Productos únicos después de quitar duplicados: 490965\n"
     ]
    }
   ],
   "source": [
    "# Filtrar para quedarnos solo con los productos NO DUPLICADOS\n",
    "df = df[~duplicados]\n",
    "print(f\" Productos únicos después de quitar duplicados: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Guardando df procesado para reutilizarlo más adelante \n",
    "\n",
    "df.to_csv('../data/processed/df_pro.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación de Índice en E.S e ingesta de documentos usando Bulk**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Como siguiente paso, definimos al indice amazon_productos con el mapeo:**\n",
    "\n",
    "- name: text (para búsquedas y análisis).\n",
    "- main_category: keyword (para filtros exactos).\n",
    "- sub_category: keyword.\n",
    "- image: keyword (no se necesita por ahora).\n",
    "- link: keyword.\n",
    "- ratings: float.\n",
    "- no_of_ratings: integer.\n",
    "- discount_price_dolares: float.\n",
    "- actual_price_dolares: float.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice amazon_products eliminado.\n",
      " Índice amazon_products creado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Coneccion a E.S\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"amazon_products\"\n",
    "\n",
    "# Configuración y Mapping\n",
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_analyzer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},  # Búsquedas flexibles por nombre\n",
    "            \"main_category\": {\"type\": \"keyword\"},                      # Filtros exactos por categoría principal\n",
    "            \"sub_category\": {\"type\": \"keyword\"},                       # Filtros exactos por subcategoría\n",
    "            \"ratings\": {\"type\": \"float\"},                              # Para ordenar por calificación\n",
    "            \"no_of_ratings\": {\"type\": \"integer\"},                      # Para ordenar por popularidad\n",
    "            \"discount_price_dolares\": {\"type\": \"float\"},               # Filtrado por precio con descuento\n",
    "            \"actual_price_dolares\": {\"type\": \"float\"}                  # Filtrado por precio original\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Crear el índice\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Índice {index_name} eliminado.\")\n",
    "\n",
    "es.indices.create(index=index_name, body=index_config)\n",
    "print(f\" Índice {index_name} creado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ingesta de datos en E.S**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def indexar_datos(es, df, index_name, chunk_size=5000):\n",
    "    # Mostrar cantidad de filas antes de la limpieza\n",
    "    print(f\" Total de filas antes de limpiar: {len(df)}\")\n",
    "    \n",
    "    # Limpiar el DataFrame y reemplazar NaNs por None\n",
    "    df_cleaned = df[['name', 'main_category', 'sub_category', 'ratings', 'no_of_ratings', \n",
    "                     'discount_price_dolares', 'actual_price_dolares', 'image', 'link']].copy()\n",
    "\n",
    "    #Al momento no existen nulos en este campo, pero se incluye el codigo por si la base de input cambia en un futuro\n",
    "    #                 \n",
    "    df_cleaned = df_cleaned.dropna(subset=['name'])\n",
    "    df_cleaned = df_cleaned.where(pd.notna(df_cleaned), None)\n",
    "    \n",
    "    #  Mostrar cantidad de filas después de la limpieza\n",
    "    print(f\" Total de filas después de limpiar: {len(df_cleaned)}\")\n",
    "    \n",
    "    #  Conversión explícita de columnas\n",
    "    df_cleaned['ratings'] = pd.to_numeric(df_cleaned['ratings'], errors='coerce').fillna(0)\n",
    "    df_cleaned['no_of_ratings'] = pd.to_numeric(df_cleaned['no_of_ratings'], errors='coerce').fillna(0)\n",
    "    df_cleaned['discount_price_dolares'] = pd.to_numeric(df_cleaned['discount_price_dolares'], errors='coerce').fillna(0)\n",
    "    df_cleaned['actual_price_dolares'] = pd.to_numeric(df_cleaned['actual_price_dolares'], errors='coerce').fillna(0)\n",
    "    \n",
    "    #  Índices de documentos fallidos\n",
    "    failed_documents = []\n",
    "\n",
    "    #  Indexar en lotes\n",
    "    total_indexed = 0\n",
    "    total_failed = 0\n",
    "\n",
    "    for start_idx in range(0, len(df_cleaned), chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, len(df_cleaned))\n",
    "        batch = df_cleaned.iloc[start_idx:end_idx].to_dict(orient=\"records\")\n",
    "        \n",
    "        # Crear el formato adecuado para la indexación\n",
    "        bulk_data = [\n",
    "            {\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": record\n",
    "            }\n",
    "            for record in batch\n",
    "        ]\n",
    "        \n",
    "        # Intentar indexar y capturar errores\n",
    "        try:\n",
    "            success, failed = helpers.bulk(es, bulk_data, stats_only=True)\n",
    "            total_indexed += success\n",
    "            total_failed += failed\n",
    "            print(f\" Lote {start_idx // chunk_size + 1} indexado correctamente: {success} documentos.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error al indexar el lote {start_idx // chunk_size + 1}: {e}\")\n",
    "            failed_documents.append({\n",
    "                \"lote\": start_idx // chunk_size + 1,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    #  Mostrar resultados finales\n",
    "    print(f\"\\n Indexación completada. Total indexados: {total_indexed}. Total fallidos: {total_failed}.\")\n",
    "    \n",
    "    # Si se obtienen errores , se guardarán en un archivo JSON para revisión\n",
    "    if failed_documents:\n",
    "        with open('errores_indexacion.json', 'w') as f:\n",
    "            json.dump(failed_documents, f, indent=4)\n",
    "        print(\" Los errores se han guardado en 'errores_indexacion.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total de filas antes de limpiar: 490965\n",
      " Total de filas después de limpiar: 490965\n",
      " Lote 1 indexado correctamente: 5000 documentos.\n",
      " Lote 2 indexado correctamente: 5000 documentos.\n",
      " Lote 3 indexado correctamente: 5000 documentos.\n",
      " Lote 4 indexado correctamente: 5000 documentos.\n",
      " Lote 5 indexado correctamente: 5000 documentos.\n",
      " Lote 6 indexado correctamente: 5000 documentos.\n",
      " Lote 7 indexado correctamente: 5000 documentos.\n",
      " Lote 8 indexado correctamente: 5000 documentos.\n",
      " Lote 9 indexado correctamente: 5000 documentos.\n",
      " Lote 10 indexado correctamente: 5000 documentos.\n",
      " Lote 11 indexado correctamente: 5000 documentos.\n",
      " Lote 12 indexado correctamente: 5000 documentos.\n",
      " Lote 13 indexado correctamente: 5000 documentos.\n",
      " Lote 14 indexado correctamente: 5000 documentos.\n",
      " Lote 15 indexado correctamente: 5000 documentos.\n",
      " Lote 16 indexado correctamente: 5000 documentos.\n",
      " Lote 17 indexado correctamente: 5000 documentos.\n",
      " Lote 18 indexado correctamente: 5000 documentos.\n",
      " Lote 19 indexado correctamente: 5000 documentos.\n",
      " Lote 20 indexado correctamente: 5000 documentos.\n",
      " Lote 21 indexado correctamente: 5000 documentos.\n",
      " Lote 22 indexado correctamente: 5000 documentos.\n",
      " Lote 23 indexado correctamente: 5000 documentos.\n",
      " Lote 24 indexado correctamente: 5000 documentos.\n",
      " Lote 25 indexado correctamente: 5000 documentos.\n",
      " Lote 26 indexado correctamente: 5000 documentos.\n",
      " Lote 27 indexado correctamente: 5000 documentos.\n",
      " Lote 28 indexado correctamente: 5000 documentos.\n",
      " Lote 29 indexado correctamente: 5000 documentos.\n",
      " Lote 30 indexado correctamente: 5000 documentos.\n",
      " Lote 31 indexado correctamente: 5000 documentos.\n",
      " Lote 32 indexado correctamente: 5000 documentos.\n",
      " Lote 33 indexado correctamente: 5000 documentos.\n",
      " Lote 34 indexado correctamente: 5000 documentos.\n",
      " Lote 35 indexado correctamente: 5000 documentos.\n",
      " Lote 36 indexado correctamente: 5000 documentos.\n",
      " Lote 37 indexado correctamente: 5000 documentos.\n",
      " Lote 38 indexado correctamente: 5000 documentos.\n",
      " Lote 39 indexado correctamente: 5000 documentos.\n",
      " Lote 40 indexado correctamente: 5000 documentos.\n",
      " Lote 41 indexado correctamente: 5000 documentos.\n",
      " Lote 42 indexado correctamente: 5000 documentos.\n",
      " Lote 43 indexado correctamente: 5000 documentos.\n",
      " Lote 44 indexado correctamente: 5000 documentos.\n",
      " Lote 45 indexado correctamente: 5000 documentos.\n",
      " Lote 46 indexado correctamente: 5000 documentos.\n",
      " Lote 47 indexado correctamente: 5000 documentos.\n",
      " Lote 48 indexado correctamente: 5000 documentos.\n",
      " Lote 49 indexado correctamente: 5000 documentos.\n",
      " Lote 50 indexado correctamente: 5000 documentos.\n",
      " Lote 51 indexado correctamente: 5000 documentos.\n",
      " Lote 52 indexado correctamente: 5000 documentos.\n",
      " Lote 53 indexado correctamente: 5000 documentos.\n",
      " Lote 54 indexado correctamente: 5000 documentos.\n",
      " Lote 55 indexado correctamente: 5000 documentos.\n",
      " Lote 56 indexado correctamente: 5000 documentos.\n",
      " Lote 57 indexado correctamente: 5000 documentos.\n",
      " Lote 58 indexado correctamente: 5000 documentos.\n",
      " Lote 59 indexado correctamente: 5000 documentos.\n",
      " Lote 60 indexado correctamente: 5000 documentos.\n",
      " Lote 61 indexado correctamente: 5000 documentos.\n",
      " Lote 62 indexado correctamente: 5000 documentos.\n",
      " Lote 63 indexado correctamente: 5000 documentos.\n",
      " Lote 64 indexado correctamente: 5000 documentos.\n",
      " Lote 65 indexado correctamente: 5000 documentos.\n",
      " Lote 66 indexado correctamente: 5000 documentos.\n",
      " Lote 67 indexado correctamente: 5000 documentos.\n",
      " Lote 68 indexado correctamente: 5000 documentos.\n",
      " Lote 69 indexado correctamente: 5000 documentos.\n",
      " Lote 70 indexado correctamente: 5000 documentos.\n",
      " Lote 71 indexado correctamente: 5000 documentos.\n",
      " Lote 72 indexado correctamente: 5000 documentos.\n",
      " Lote 73 indexado correctamente: 5000 documentos.\n",
      " Lote 74 indexado correctamente: 5000 documentos.\n",
      " Lote 75 indexado correctamente: 5000 documentos.\n",
      " Lote 76 indexado correctamente: 5000 documentos.\n",
      " Lote 77 indexado correctamente: 5000 documentos.\n",
      " Lote 78 indexado correctamente: 5000 documentos.\n",
      " Lote 79 indexado correctamente: 5000 documentos.\n",
      " Lote 80 indexado correctamente: 5000 documentos.\n",
      " Lote 81 indexado correctamente: 5000 documentos.\n",
      " Lote 82 indexado correctamente: 5000 documentos.\n",
      " Lote 83 indexado correctamente: 5000 documentos.\n",
      " Lote 84 indexado correctamente: 5000 documentos.\n",
      " Lote 85 indexado correctamente: 5000 documentos.\n",
      " Lote 86 indexado correctamente: 5000 documentos.\n",
      " Lote 87 indexado correctamente: 5000 documentos.\n",
      " Lote 88 indexado correctamente: 5000 documentos.\n",
      " Lote 89 indexado correctamente: 5000 documentos.\n",
      " Lote 90 indexado correctamente: 5000 documentos.\n",
      " Lote 91 indexado correctamente: 5000 documentos.\n",
      " Lote 92 indexado correctamente: 5000 documentos.\n",
      " Lote 93 indexado correctamente: 5000 documentos.\n",
      " Lote 94 indexado correctamente: 5000 documentos.\n",
      " Lote 95 indexado correctamente: 5000 documentos.\n",
      " Lote 96 indexado correctamente: 5000 documentos.\n",
      " Lote 97 indexado correctamente: 5000 documentos.\n",
      " Lote 98 indexado correctamente: 5000 documentos.\n",
      " Lote 99 indexado correctamente: 965 documentos.\n",
      "\n",
      " Indexación completada. Total indexados: 490965. Total fallidos: 0.\n",
      " Documentos indexados: 490965\n"
     ]
    }
   ],
   "source": [
    "# Conexión con Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Nombre del índice\n",
    "index_name = \"amazon_products\"\n",
    "\n",
    "# Ejecutar la función de indexación\n",
    "indexar_datos(es, df, index_name)\n",
    "\n",
    "# Verificar cantidad de documentos indexados\n",
    "doc_count = es.count(index=index_name)['count']\n",
    "print(f\" Documentos indexados: {doc_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
